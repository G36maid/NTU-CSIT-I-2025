# Week 9: Trustworthy AI

- **授課教師**: 羅紹元 (Shao-Yuan Lo)
- **主題**: An overview of Trustworthy AI, focusing on adversarial robustness and the application of Multimodal Large Language Models (MLLMs) in AI safety.

本週課程由羅紹元教授介紹可信賴 AI 的範疇，深入探討了對抗性攻擊與防禦，並分享其團隊近期在多模態大型語言模型 (MLLM) 於 AI 安全應用的最新研究。

---

## 1. 可信賴 AI (Trustworthy AI) 範疇

隨著 AI 深度融入人類社會，其帶來的風險也日益增加。可信賴 AI 的研究旨在確保 AI 能安全、正面地影響人類。其研究範疇主要分為四大支柱：

1.  **安全 (Safety)**: 確保 AI 系統在各種情況下都能正常運作，免受惡意攻擊。
    - **核心議題**: 對抗性穩健性 (Adversarial Robustness)。
2.  **監控 (Monitoring)**: 賦予 AI 系統監測環境、預測行為、偵測異常的能力。
    - **核心議題**: 異常偵測 (Anomaly Detection)。
3.  **倫理 (Ethics)**: 確保 AI 的設計與決策符合道德標準。
    - **核心議題**: 隱私 (Privacy), 公平性 (Fairness), 透明度 (Transparency)。
4.  **對齊 (Alignment)**: 確保 AI 的目標與人類的價值觀和意圖保持一致。
    - **核心議題**: 人類價值觀 (Human Values), 人機互動 (Human-AI Interaction)。

---

## 2. 安全性：對抗性穩健性 (Adversarial Robustness)

### 什麼是對抗性樣本 (Adversarial Example)？

深度神經網路容易受到對抗性樣本的攻擊。這類樣本是在原始輸入（如影像）上添加了人眼難以察覺的微小擾動 (perturbation) `δ`，但卻能讓模型產生完全錯誤的輸出。

- **數學表示**: `x_adv = x + δ`，使得 `f(x_adv) ≠ y`。
- **生成方式**: 一般是透過**最大化**模型的損失函數來找到能造成最大混淆的擾動 `δ`，與模型訓練時**最小化**損失函數的目標正好相反。
- **攻擊類型**:
    - **非目標式攻擊 (Untargeted)**: 讓模型輸出**任何**錯誤的標籤即可。
    - **目標式攻擊 (Targeted)**: 讓模型輸出一個**指定**的錯誤標籤。
- **攻擊知識**:
    - **白箱攻擊 (White-box)**: 攻擊者完全了解模型的架構與參數。
    - **黑箱攻擊 (Black-box)**: 攻擊者只能存取模型的輸入與輸出。

### 如何防禦對抗性攻擊？

- **影像轉換**: 在輸入端對影像進行處理（如去噪），試圖移除擾動。此為治標方法。
- **對抗性訓練 (Adversarial Training, AT)**: 在訓練過程中，不斷生成對抗性樣本，並將其納入訓練集中，強迫模型學習如何在存在擾動的情況下做出正確判斷。此為治本方法。

對抗性訓練是目前最有效的防禦手段之一。它能顯著提升模型在面對**已知**類型攻擊時的穩健性 (Robust Accuracy)，但通常會犧牲一些在乾淨樣本上的準確率 (Clean Accuracy)，這是一個已知的**權衡 (Trade-off)**。

### 挑戰：多重擾動穩健性 (Multi-Perturbation Robustness)

標準的對抗性訓練在面對多種類型的攻擊時效果不佳。若模型僅針對 PGD 攻擊進行訓練，它對其他類型的攻擊（如遮擋、雜訊）的防禦力依然很弱。

- **觀察**: 不同類型的對抗性樣本與乾淨樣本，其在模型中的特徵呈現出**顯著不同的數據分佈**。
- **問題**: 當使用混合了多種攻擊樣本的數據進行訓練時，模型中的批次正規化層 (Batch Normalization, BN) 會因為面對混合分佈而感到「困惑」，導致學習效果不佳。
- **解決方案：MultiBN**:
    - 提出一種**多 BN 層結構**，為每一種數據分佈（乾淨樣本、PGD 攻擊、遮擋攻擊等）分配一個專屬的 BN 層。
    - 這種架構類似於**混合專家模型 (Mixture-of-Experts, MoE)**，讓模型能針對不同分佈的數據進行專門的正規化處理。
    - 在推論時，設計一個 BN 選擇模組，自動為輸入數據選擇最合適的 BN 分支。
    - **成果**: MultiBN 在多重擾動防禦任務上，相較於傳統的混合對抗性訓練，穩健準確率提升了 **10 倍**。

### 其他安全性威脅

- **毒化攻擊 (Poisoning Attacks)**: 在訓練數據中注入惡意樣本，以破壞模型的決策邊界。
- **後門攻擊 (Backdoor Attacks)**: 一種特殊的毒化攻擊，在模型中植入一個「觸發器」(trigger)。平時模型表現正常，一旦輸入包含觸發器，就會產生攻擊者預設的錯誤輸出。
- **成員推斷攻擊 (Membership Inference Attacks)**: 判斷某個特定數據樣本是否曾被用於模型的訓練，可能洩露敏感資訊。
- **模型提取攻擊 (Extraction Attacks)**: 透過不斷查詢模型的輸出，逆向複製出一個功能相似的模型副本。

---

## 3. 近期研究：應用 MLLM 於可信賴 AI

羅教授的實驗室近期專注於利用多模態大型語言模型 (MLLM) 的強大能力，來解決可信賴 AI 中的監控與對齊問題。

### 應用於監控：異常偵測 (Anomaly Detection, AD)

傳統的 AD 方法需要大量的正常樣本進行訓練 (Full-shot)，且只能輸出一個異常分數。MLLM 為 AD 帶來了新的可能性：

- **學習方式**: 從 Full-shot 轉向 **Few-shot / Zero-shot**，僅需少量甚至無需樣本即可適應新任務。
- **輸出內容**: 從單純的異常分數，進化為**偵測 + 推理 (Detection + Reasoning)**，能解釋為何判斷為異常。

#### 研究一：影片異常偵測 (VAD) (ECCV 2024)

- **方法**: 提出一種兩階段的 Few-shot prompting 方法，無需訓練。
    1.  **歸納 (Induction)**: 給予 MLLM 少數幾個「正常」的影片影格作為參考。
    2.  **演繹 (Deduction)**: MLLM 根據歸納出的「正常」規則，來判斷新的查詢影片是否異常，並生成解釋。
- **成果**: 在 challenging 的 VAD 資料集上，其 AUROC 表現超越了許多需要大量訓練的傳統模型。

#### 研究二：影像異常偵測 (IAD) (CVPR 2025 Highlight)

- **目標**: 實現 Zero-shot 的 IAD，並生成高品質的推理依據。
- **貢獻**:
    - 提出一個**兩步檢測**框架，模擬人類的視覺檢查流程。
    - 構建了全球首個用於 IAD 推理的**視覺指令微調 (Visual Instruction Tuning)** 資料集 `Anomaly-Instruct-125k`，包含 7.2 萬張網頁圖片和多個現有 AD 資料集，用來教導 MLLM 如何進行 AD 相關的推理。

### 應用於對齊：情感推理與心智理論

- **影片情感推理 (Video Affective Reasoning, VAR) (IJCV 2025)**:
    - 旨在預測並解釋觀眾觀看影片時可能產生的情緒反應。
    - 提出**時空刺激感知機制**，讓模型能自動關注影片中引發情緒的關鍵事件與物體，從而做出更準確的預測和解釋。

- **多模態心智理論 (Multimodal Theory-of-Mind, MMToM) (ICML 2025 Spotlight)**:
    - **心智理論 (ToM)**: 理解他人心理狀態（如意圖、信念、情感）的能力。
    - **目標**: 將此能力賦予 AI，使其能更好地理解人類。
    - **挑戰**: 訓練 MLLM 具備 MMToM 能力非常昂貴。
    - **貢獻**: 提出一種**由弱至強的控制 (Weak-to-Strong Control)** 策略，僅需訓練一個較小的 LM，就能將其學到的 ToM 對齊能力遷移到一個未經訓練的、更強大的 LM 上，大幅降低了訓練成本。